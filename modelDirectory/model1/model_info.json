{
    "model_name": "DeBERTa-v3-base",
    "epochs": 5,
    "learning_rate": 5e-05,
    "learning_rate_scheduler": "linear",
    "batch_size": 8,
    "max_sequence_length": 256,
    "notes": "This is the basic model with the first parameters i tried that would fit into 8GB VRAM."
}